# -*- coding: utf-8 -*-
"""mBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SxZ6dvLaGK6gx4cIbbvCmfzusWVP_EsO
"""

from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import TensorDataset, DataLoader
import torch
from torch.optim import AdamW
import torch.nn as nn
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from transformers import BertTokenizer

df = pd.read_excel("/content/myFinalDataset(edited version).xlsx")

# Preprocess Data
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
train_df = train_df.dropna(subset=['label']).reset_index(drop=True)
train_df['label'] = train_df['label'].astype(int)
test_df['label'] = test_df['label'].astype(int)

# Tokenization
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
train_encodings = tokenizer(train_df['text'].tolist(), truncation=True, padding=True)
test_encodings = tokenizer(test_df['text'].tolist(), truncation=True, padding=True)

# Create TensorDatasets for train dataset
train_input_ids = torch.tensor(train_encodings['input_ids'])
train_attention_mask = torch.tensor(train_encodings['attention_mask'])
train_labels = torch.tensor(train_df['label'].values)  # Convert to numpy array to avoid shape issues

train_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)

# Create TensorDatasets for test dataset
test_input_ids = torch.tensor(test_encodings['input_ids'])
test_attention_mask = torch.tensor(test_encodings['attention_mask'])
test_labels = torch.tensor(test_df['label'].values)  # Convert to numpy array to avoid shape issues

test_dataset = TensorDataset(test_input_ids, test_attention_mask, test_labels)


from torch.cuda.amp import autocast, GradScaler

# Define batch size and gradient accumulation steps
batch_size = 8
accumulation_steps = 4

# Define DataLoader for train dataset with reduced batch size
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Define DataLoader for test dataset with reduced batch size
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

import torch

# Define the device (GPU if available, else CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load model
model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)
model.to(device)  # Move model to GPU if available

# Define optimizer and loss function
optimizer = AdamW(model.parameters(), lr=1e-5)
loss_fn = nn.CrossEntropyLoss()

# Initialize gradient scaler for mixed precision training
scaler = GradScaler()

# Training loop
epochs = 1

for epoch in range(epochs):
    model.train()
    total_loss = 0
    for step, batch in enumerate(train_loader):
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        optimizer.zero_grad()

        with autocast():  # Automatic mixed precision training
            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss / accumulation_steps
            scaler.scale(loss).backward()

        if (step + 1) % accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

        total_loss += loss.item()

    # Print average loss for each epoch
    print(f"Epoch {epoch+1}, Average Loss: {total_loss / len(train_loader)}")

# Evaluation
model.eval()
with torch.no_grad():
    total_predictions = []
    total_labels = []
    for batch in test_loader:
        input_ids, attention_mask, labels = batch
        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)

        outputs = model(input_ids, attention_mask=attention_mask)
        logits = outputs.logits
        predictions = torch.argmax(logits, dim=1)
        total_predictions.extend(predictions.tolist())
        total_labels.extend(labels.tolist())

accuracy = accuracy_score(total_labels, total_predictions)
print("Test Accuracy:", accuracy)